# Licensed under a 3-clause BSD style license - see LICENSE.rst
# -*- coding: utf-8 -*-
"""
==================
desitarget.QA
==================

Module dealing with Quality Assurance tests for Target Selection
"""
from __future__ import (absolute_import, division)
#
from time import time
import numpy as np
import fitsio
import os, re
from collections import defaultdict
from glob import glob
from scipy.optimize import leastsq
from scipy.spatial import ConvexHull
import random

from . import __version__ as desitarget_version

from desiutil import depend
from desiutil.log import get_logger, DEBUG
import warnings

def sph2car(ra, dec):
    """Convert RA and Dec to a Cartesian vector """

    phi = np.radians(np.asarray(ra))
    theta = np.radians(90.0 - np.asarray(dec))

    r = np.sin(theta)
    x = r * np.cos(phi)
    y = r * np.sin(phi)
    z = np.cos(theta)

    #ADM treat vectors smaller than our tolerance as zero
    tol = 1e-15
    x = np.where(np.abs(x) > tol,x,0)
    y = np.where(np.abs(y) > tol,y,0)
    z = np.where(np.abs(z) > tol,z,0)

    return np.array((x, y, z)).T


def area_of_hull(ras,decs,nhulls):
    """Determine the area of a convex hull from an array of RA, Dec points that define that hull
    
    Parameters
    ----------
    ras : :class:`~numpy.array`
        Right Ascensions of the points in the convex hull (boundary) in DEGREES
    decs : :class:`~numpy.array`
        Declinations of the points in the convex hull (boundary) in DEGREES
    nhulls : :class:`int`
        The number of hulls the user thinks they passed, to check each hull occupies a row
        not a column

    Returns
    -------
    :class:`float`
        The area of the convex hull (drawn on the surface of the sphere) in SQUARE DEGREES

    Notes
    -----
        - If a 2-D array of RAs and Decs representing multiple convex hulls is passed, than the 
          output will be a 1-D array of areas. Each hull must occupy the rows, e.g. for a
          set of RAs for 2 identical hulls with 15 points on the boundary, this would be the
          correct array ordering:

          array([[ 7.14639156,  7.02689545,  7.01554989,  7.01027328,  7.01276138,
                   7.01444518,  7.0173733 ,  7.03278736,  7.22537629,  7.24887231,
                   7.25749704,  7.25629861,  7.25075961,  7.24776393,  7.2375672 ],
                 [ 7.14639156,  7.02689545,  7.01554989,  7.01027328,  7.01276138,
                   7.01444518,  7.0173733 ,  7.03278736,  7.22537629,  7.24887231,
                   7.25749704,  7.25629861,  7.25075961,  7.24776393,  7.2375672 ]])

        - This is only an approximation, because it uses the average latitude. See, e.g.:

              https://trs.jpl.nasa.gov/bitstream/handle/2014/41271/07-0286.pdf

          but it's accurate (to millionths of a per cent) for areas of a few sq. deg.

        - This routine will fail at the poles. So, decs should never be passed as -90. or 90.
    """

    if ras.ndim > 1 and ras.shape[0] != nhulls:
        raise IOError('Your array contains {} hulls. Perhaps you meant to pass its tranposition?'.format(ras.shape[0]))

    #ADM try to catch polar cases (this won't catch areas of larger than a few degrees,
    #ADM but this routine is inaccurate for large hulls anyway)
    if np.max(np.abs(decs)) >= 89.:
        raise IOError('You passed a declination of > 89o or < -89o. This routine does not work at or over the poles')
    
    #ADM ensure RAs run from 0 -> 360
    ras%=360

    #ADM we'll loop over pairs of vertices around the hull
    startras = np.roll(ras,+1,axis=1)
    endras = np.roll(ras,-1,axis=1)
    
    rawidth = startras-endras
    #ADM To deal with wraparound issues, assume that any "large" RA intervals cross RA=0
    w = np.where(rawidth < -180.)
    rawidth[w] -= -360.
    w = np.where(rawidth > 180.)
    rawidth[w] -= 360.

    spharea = np.abs(0.5*np.sum(rawidth*np.degrees(np.sin(np.radians(decs))),axis=1))

    return spharea


def targets_on_hull(targets,downsample=False):
    """Create the convex hull (boundary) of an area from an input rec array of targets
    
    Parameters
    ----------
    targets : :class:`recarray`
        File of targets generated by `desitarget.bin.select_targets`
    downsample : :class: `boolean`
        If True, downsample the input target file (randomly) to 2 million objects in order
        to speed the calculation of the boundary

    Returns
    -------
    :class:`integer`
        The INDICES of the input target file that contain the RA/Dec of the targets that form the
        vertices of the boundary in counter-clockwise order

    Notes
    -----
    It takes about 30 seconds to run this on Edison for an input rec array of 2 million objects (i.e. 
    with downsample=True)
    """

    ra = targets["RA"]
    dec = targets["DEC"]

    #ADM either pick every index in the targets array, or downsample to 2 million indices
    isamp = np.arange(len(ra))
    if downsample:
        isamp = np.array(random.sample(range(len(ra)),2000000))

    #ADM the 2 restricts to 2D (i.e. the RA/Dec plane)
    hull = ConvexHull(np.vstack(zip(ra[isamp],dec[isamp])))

    return isamp[hull.vertices]
    

def is_polygon_within_boundary(boundverts,polyverts):
    """Check whether a list of polygons are within the boundary of a convex hull

    Parameters
    ----------
    boundverts : :class:`float array`
       A COUNTER-CLOCKWISE ordered array of vectors representing the Cartesian coordinates of the vertices 
       of the complex hull (boundary) of a survey, e.g.

       array([[x1,  y1,  z1],      Vertex 1 on survey boundary
              [x2,  y2,  z2],      Vertex 2 of survey boundary
              [x3,  y3,  z3],      Vertex 3 of survey boundary
              [x4,  y4,  z4],      Vertex 4 of survey boundary
              [x5,  y5,  z5],      Vertex 5 of survey boundary

              ..............
              ..............
              ..............

              [xN,  yN,  zN]])     Vertex N of survey boundary
    polyverts : :class:`float array'`
       An array of N M-vectors representing the Cartesian coordinates of the vertices of each
       polygon. For instance, M=4 would represent "rectangular" vertices drawn on the sphere:

       array([[[x1,  y1,  z1],       Vertex 1 of first Rectangle
               [x2,  y2,  z2],       Vertex 2 of first Rectangle
               [x3,  y3,  z3],       Vertex 3 of first Rectangle
               [x4,  y4,  z4]],      Vertex 4 of first Rectangle

               ..............
               ..............
               ..............

              [[Nx1, Ny1,  Nz1],     Vertex 1 of Nth Rectangle
               [Nx2, Ny2,  Nz2],     Vertex 2 of Nth Rectangle
               [Nx3, Ny3,  Nz3],     Vertex 3 of Nth Rectangle
               [Nx4, Ny4,  Nz4]]])   Vertex 4 of Nth Rectangle

    Returns
    -------
    :class:`boolean array`
       An array the same length as polyvertices (i.e. the "number of rectangles") that is True for
       polygons that are FULLY within the boundary and False for polygons that are not within the boundary

    Notes
    -----
    Strictly, as we only test whether the vertices of the polygons are all within the boundary, there can be 
    corner cases where a small amount of a polygon edge intersects the boundary. Testing the vertices should be good 
    enough for desitarget QA, though.
    """

    #ADM recast inputs as float64 as the inner1d ufunc does not support higher-bit floats
    boundverts = boundverts.astype('f8')
    polyverts = polyverts.astype('f8')

    #ADM an array of Trues and Falses for the output. Default to True.
    boolwithin = np.ones(len(polyverts),dtype='bool')

    #ADM loop through each of the boundary vertices, starting with the final vertex to ensure that
    #ADM we traverse the entire boundary. Could make this faster by dropping polygons as soon as we
    #ADM find a negative vertex, but it's fairly quick as is
    for i in range(-1,len(boundverts)-1):
        #ADM The algorithm is to check the direction of the projection (dot product) of each vertex of each polygon
        #ADM onto each vector normal (cross product) to the geodesics (planes) that map out the survey boundary
        #ADM If this projection is positive, then we "turned left" to get to the polygon vertex, assuming the convex
        #ADM hull is ordered counter-clockwise
        test = inner1d(np.cross(boundverts[i],boundverts[i+1]),polyverts)
        #ADM if any of the vertices are not a left-turn from (within the) boundary points, set to False
        boolwithin[np.where(np.any(test < 0, axis=1))] = False

    return boolwithin


def generate_fluctuations(brickfilename, targettype, depthtype, depthorebvarray, random_state=None):
    """Based on depth or E(B-V) values for a brick, generate target fluctuations

    Parameters
    ----------
    brickfilename : :class:`str`
        File name of a list of a brick info file made by QA.brick_info.
    targettype : :class: `str`
        Name of the target type for which to generate fluctuations
        options are "ALL", "LYA", "MWS", "BGS", "QSO", "ELG", "LRG"
    depthtype : :class: `str`
        Name of the type of depth-and-band (or E(B-V)) for which to generate fluctuations
        options are "PSFDEPTH_G", "GALDEPTH_G", "PSFDEPTH_R", "GALDEPTH_R", "PSFDEPTH_Z", "GALDEPTH_Z"
        or pass "EBV" to generate fluctuations based off E(B-V) values
    depthorebvarray : :class:`float array`
        An array of brick depths (i.e. for N bricks, N realistic values of depth) or
        an array of E(B-V) values if EBV is passed

    Returns
    -------
    :class:`float`
        An array of the same length as depthorebvarray with per-brick fluctuations
        generated from actual DECaLS data

    """
    if random_state is None:
        random_state = np.random.RandomState()
        
    log = get_logger()

    #ADM check some impacts are as expected
    dts = ["PSFDEPTH_G","GALDEPTH_G","PSFDEPTH_R","GALDEPTH_R","PSFDEPTH_Z","GALDEPTH_Z","EBV"]
    if not depthtype in dts:
        raise ValueError("depthtype must be one of {}".format(" ".join(dts)))

    #if depthtype == "EBV":
        #print("generating per-brick fluctuations for E(B-V) values")
    #else:
        #print("generating per-brick fluctuations for depth values")

    if not type(depthorebvarray) == np.ndarray:
        raise ValueError("depthorebvarray must be a numpy array not type {}".format(type(depthorebvarray)))

    #ADM the number of bricks
    nbricks = len(depthorebvarray)
    tts = ["ALL","LYA","MWS","BGS","QSO","ELG","LRG"]
    if not targettype in tts:
        fluc = np.ones(nbricks)
        mess = "fluctuations for targettype {} are set to one".format(targettype)
        log.warning(mess)
        return fluc

    #ADM the target fluctuations are actually called FLUC_* in the model dictionary
    targettype = "FLUC_"+targettype

    #ADM generate/retrieve the model map dictionary
    modelmap = model_map(brickfilename)

    #ADM retrive the quadratic model
    means, sigmas = modelmap[depthtype][targettype]

    #ADM sample the distribution for each parameter in the quadratic
    #ADM fit for each of the total number of bricks
    asamp = random_state.normal(means[0], sigmas[0], nbricks)
    bsamp = random_state.normal(means[1], sigmas[1], nbricks)
    csamp = random_state.normal(means[2], sigmas[2], nbricks)

    #ADM grab the fluctuation in each brick
    fluc = asamp*depthorebvarray**2. + bsamp*depthorebvarray + csamp

    return fluc

def model_map(brickfilename,plot=False):
    """Make a model map of how 16,50,84 percentiles of brick depths and how targets fluctuate with brick depth and E(B-V)

    Parameters
    ----------
    brickfilename : :class:`str`
        File name of a list of a brick info file made by QA.brick_info.
    plot : :class:`boolean`, optional
        generate a plot of the data and the model if True

    Returns
    -------
    :class:`dictionary`
        model of brick fluctuations, median, 16%, 84% for overall
        brick depth variations and variation of target density with
        depth and EBV (per band). The first level of the nested
        dictionary are the keys PSFDEPTH_G, PSFDEPTH_R, PSFDEPTH_Z, GALDEPTH_G,
        GALDEPTH_R, GALDEPTH_Z, EBV. The second level are the keys PERC,
        which contains a list of values corresponding to the 16,50,84%
        fluctuations in that value of DEPTH or EBV per brick and the
        keys corresponding to each target_class, which contain a list
        of values [a,b,c,da,db,dc] which correspond to a quadratic model
        for the fluctuation of that target class in the form y = ax^2 +bx + c
        together with the errors on a, b and c. Here, y would be the
        target density fluctuations and x would be the DEPTH or EBV value.

    """
    flucmap = fluc_map(brickfilename)

    #ADM the percentiles to consider for "mean" and "sigma
    percs = [16,50,84]

    firstcol = 1
    cols = flucmap.dtype.names
    for col in cols:
        #ADM loop through each of the depth columns (PSF and GAL) and EBV
        if re.search("PSFDEPTH",col) or re.search("EBV",col):
            #ADM the percentiles for this DEPTH/EBV across the brick
            coldict = {"PERCS": np.percentile(flucmap[col],percs)}
            #ADM fit quadratic models to target density fluctuation vs. EBV and
            #ADM target density fluctuation vs. depth
            for fcol in cols:
                if re.search("FLUC",fcol):
                    if plot:
                        print("doing",col,fcol)
                    quadparams = fit_quad(flucmap[col],flucmap[fcol],plot=plot)
                    #ADD this to the dictionary
                    coldict = dict({fcol:quadparams},**coldict)
            #ADM nest the information in an overall dictionary corresponding to
            #ADM this column name
            flucdict = {col:coldict}
            #ADM first time through set up the dictionary and
            #ADM on subsequent loops add to it
            if firstcol:
                outdict = flucdict
                firstcol = 0
            else:
                outdict = dict(flucdict,**outdict)

    return outdict


def fit_quad(x,y,plot=False):
    """Fit a quadratic model to (x,y) ordered data.

    Parameters
    ----------
    x : :class:`float`
        x-values of data (for typical x/y plot definition)
    y : :class:`float`
        y-values of data (for typical x/y plot definition)
    plot : :class:`boolean`, optional
        generate a plot of the data and the model if True

    Returns
    -------
    params :class:`3-float`
        The values of a, b, c in the typical quadratic equation
        y = ax^2 + bx + c
    errs :class:`3-float`
        The error on the fit of each parameter
    """

    #ADM standard equation for a quadratic
    funcQuad = lambda params,x : params[0]*x**2+params[1]*x+params[2]
    #ADM difference between model and data
    errfunc = lambda params,x,y: funcQuad(params,x)-y
    #ADM initial guesses at params
    initparams = (1.,1.,1.)
    #ADM loop to get least squares fit
    with warnings.catch_warnings(): # added by Moustakas
        warnings.simplefilter('ignore')

        params,ok = leastsq(errfunc,initparams[:],args=(x,y))
        params, cov, infodict, errmsg, ok = leastsq(errfunc, initparams[:], args=(x, y),
                                                    full_output=1, epsfcn=0.0001)

    #ADM turn the covariance matrix into something chi-sq like
    #ADM via degrees of freedom
    if (len(y) > len(initparams)) and cov is not None:
        s_sq = (errfunc(params, x, y)**2).sum()/(len(y)-len(initparams))
        cov = cov * s_sq
    else:
        cov = np.inf

    #ADM estimate the error on the fit from the diagonal of the covariance matrix
    err = []
    for i in range(len(params)):
        try:
          err.append(np.absolute(cov[i][i])**0.5)
        except:
          err.append(0.)
    err = np.array(err)

    if plot:
        import matplotlib.pyplot as plt
        #ADM generate a model
        step = 0.01*(max(x)-min(x))
        xmod = step*np.arange(100)+min(x)
        ymod = xmod*xmod*params[0] + xmod*params[1] + params[2]
        #ADM rough upper and lower bounds from the errors
#        ymodhi = xmod*xmod*(params[0]+err[0]) + xmod*(params[1]+err[1]) + (params[2]+err[2])
#        ymodlo = xmod*xmod*(params[0]-err[0]) + xmod*(params[1]-err[1]) + (params[2]-err[2])
        ymodhi = xmod*xmod*params[0] + xmod*params[1] + (params[2]+err[2])
        ymodlo = xmod*xmod*params[0] + xmod*params[1] + (params[2]-err[2])
        #ADM axes that clip extreme outliers
        plt.axis([np.percentile(x,0.1),np.percentile(x,99.9),
                  np.percentile(y,0.1),np.percentile(y,99.9)])
        plt.plot(x,y,'k.',xmod,ymod,'b-',xmod,ymodhi,'b.',xmod,ymodlo,'b.')
        plt.show()

    return params, err


def fluc_map(brickfilename):
    """From a brick info file (as in construct_QA_file) create a file of target fluctuations

    Parameters
    ----------
    brickfilename : :class:`str`
        File name of a list of a brick info file made by QA.brick_info.

    Returns
    -------
    :class:`recarray`
        numpy structured array of number of times median target density
        for each brick for which NEXP is at least 3 in all of g/r/z bands.
        Contains EBV and pixel-weighted mean depth for building models
        of how target density fluctuates.
    """

    #ADM reading in brick_info file
    fx = fitsio.FITS(brickfilename, upper=True)
    alldata = fx[1].read()

    #ADM limit to just things with NEXP=3 in every band
    #ADM and that have reasonable depth values from the depth maps
    try:
        w = np.where( (alldata['NEXP_G'] > 2) & (alldata['NEXP_R'] > 2) & (alldata['NEXP_Z'] > 2) &
                      (alldata['PSFDEPTH_G'] > -90) & (alldata['PSFDEPTH_R'] > -90) & (alldata['PSFDEPTH_Z'] > -90))
    except:
        w = np.where( (alldata['NEXP_G'] > 2) & (alldata['NEXP_R'] > 2) & (alldata['NEXP_Z'] > 2) &
                      (alldata['DEPTH_G'] > -90) & (alldata['DEPTH_R'] > -90) & (alldata['DEPTH_Z'] > -90))
    alldata = alldata[w]

    #ADM choose some necessary columns and rename density columns,
    #ADM which we'll now base on fluctuations around the median

    #JM -- This will only work for DR3!
    cols = [
            'BRICKID','BRICKNAME','BRICKAREA','RA','DEC','EBV',
            'DEPTH_G','DEPTH_R','DEPTH_Z',
            'GALDEPTH_G', 'GALDEPTH_R', 'GALDEPTH_Z',
            'DENSITY_ALL', 'DENSITY_ELG', 'DENSITY_LRG',
            'DENSITY_QSO', 'DENSITY_LYA', 'DENSITY_BGS', 'DENSITY_MWS'
            ]
    data = alldata[cols]
    #newcols = [col.replace('DENSITY', 'FLUC') for col in cols]
    newcols = [
            'BRICKID','BRICKNAME','BRICKAREA','RA','DEC','EBV',
            'PSFDEPTH_G','PSFDEPTH_R','PSFDEPTH_Z',
            'GALDEPTH_G', 'GALDEPTH_R', 'GALDEPTH_Z',
            'FLUC_ALL', 'FLUC_ELG', 'FLUC_LRG',
            'FLUC_QSO', 'FLUC_LYA', 'FLUC_BGS', 'FLUC_MWS'
            ]
    data.dtype.names = newcols

    #ADM for each of the density columns loop through and replace
    #ADM density by value relative to median
    outdata = data.copy()
    for col in newcols:
        if re.search("FLUC",col):
            med = np.median(data[col])
            if med > 0:
                outdata[col] = data[col]/med
            else:
                outdata[col] = 1.

    return outdata


def mag_histogram(targetfilename,binsize,outfile):
    """Detemine the magnitude distribution of targets

    Parameters
    ----------
    targetfilename : :class:`str`
        File name of a list of targets created by select_targets
    binsize : :class:`float`
        bin size of the output histogram
    outfilename: :class:`str`
        Output file name for the magnitude histograms, which will be written as ASCII

    Returns
    -------
    :class:`Nonetype`
        No return...but prints a raw N(m) to screen for each target type
    """

    #ADM read in target file
    print('Reading in targets file')
    fx = fitsio.FITS(targetfilename, upper=True)
    targetdata = fx[1].read(columns=['BRICKID','DESI_TARGET','BGS_TARGET','MWS_TARGET','DECAM_FLUX'])

    #ADM open output file for writing
    file = open(outfile, "w")

    #ADM calculate the magnitudes of interest
    print('Calculating magnitudes')
    gfluxes = targetdata["DECAM_FLUX"][...,1]
    gmags = 22.5-2.5*np.log10(gfluxes*(gfluxes  > 1e-5) + 1e-5*(gfluxes < 1e-5))
    rfluxes = targetdata["DECAM_FLUX"][...,2]
    rmags = 22.5-2.5*np.log10(rfluxes*(rfluxes  > 1e-5) + 1e-5*(rfluxes < 1e-5))
    zfluxes = targetdata["DECAM_FLUX"][...,4]
    zmags = 22.5-2.5*np.log10(zfluxes*(zfluxes  > 1e-5) + 1e-5*(zfluxes < 1e-5))

    bitnames = ["ALL","LRG","ELG","QSO","BGS","MWS"]
    bitvals = [-1]+list(2**np.array([0,1,2,60,61]))

    #ADM set up bin edges in magnitude from 15 to 25 at resolution of binsize
    binedges = np.arange(((25.-15.)/binsize)+1)*binsize + 15

    #ADM loop through bits and print histogram of raw target numbers per magnitude
    for i, bitval in enumerate(bitvals):
        print('Doing',bitnames[i])
        w = np.where(targetdata["DESI_TARGET"] & bitval)
        if len(w[0]):
            ghist,dum = np.histogram(gmags[w],bins=binedges)
            rhist,dum = np.histogram(rmags[w],bins=binedges)
            zhist,dum = np.histogram(zmags[w],bins=binedges)
            file.write('{}    {}     {}     {}\n'.format(bitnames[i],'g','r','z'))
            for i in range(len(binedges)-1):
                outs = '{:.1f} {} {} {}\n'.format(0.5*(binedges[i]+binedges[i+1]),ghist[i],rhist[i],zhist[i])
                print(outs)
                file.write(outs)

    file.close()

    return None


def construct_QA_file(nrows):
    """Create a recarray to be populated with QA information

    Parameters
    ----------
    nrows : :class:`int`
        Number of rows in the recarray (size, in rows, of expected fits output)

    Returns
    -------
    :class:`recarray`
         numpy structured array of brick information with nrows as specified
         and columns as below
    """

    data = np.zeros(nrows, dtype=[
            ('BRICKID','>i4'),('BRICKNAME','S8'),('BRICKAREA','>f4'),
            ('RA','>f4'),('DEC','>f4'),
            ('RA1','>f4'),('RA2','>f4'),
            ('DEC1','>f4'),('DEC2','>f4'),
            ('EBV','>f4'),
            ('PSFDEPTH_G','>f4'),('PSFDEPTH_R','>f4'),('PSFDEPTH_Z','>f4'),
            ('GALDEPTH_G','>f4'),('GALDEPTH_R','>f4'),('GALDEPTH_Z','>f4'),
            ('PSFDEPTH_G_PERCENTILES','f4',(5)), ('PSFDEPTH_R_PERCENTILES','f4',(5)),
            ('PSFDEPTH_Z_PERCENTILES','f4',(5)), ('GALDEPTH_G_PERCENTILES','f4',(5)),
            ('GALDEPTH_R_PERCENTILES','f4',(5)), ('GALDEPTH_Z_PERCENTILES','f4',(5)),
            ('NEXP_G','i2'),('NEXP_R','i2'),('NEXP_Z','i2'),
            ('DENSITY_ALL','>f4'),
            ('DENSITY_ELG','>f4'),('DENSITY_LRG','>f4'),
            ('DENSITY_QSO','>f4'),('DENSITY_LYA','>f4'),
            ('DENSITY_BGS','>f4'),('DENSITY_MWS','>f4'),
            ('DENSITY_BAD_ELG','>f4'),('DENSITY_BAD_LRG','>f4'),
            ('DENSITY_BAD_QSO','>f4'),('DENSITY_BAD_LYA','>f4'),
            ('DENSITY_BAD_BGS','>f4'),('DENSITY_BAD_MWS','>f4'),
            ])
    return data

def populate_brick_info(instruc,brickids,rootdirname='/global/project/projectdirs/cosmo/data/legacysurvey/dr3/'):
    """Add brick-related information to a numpy array of brickids

    Parameters
    ----------
    instruc : :class:`recarray`
        numpy structured array containing at least
        ['BRICKNAME','BRICKID','RA','DEC','RA1','RA2','DEC1','DEC2',
        'NEXP_G','NEXP_R', NEXP_Z','EBV']) to populate
    brickids : :class:`recarray`
        numpy structured array (single list) of BRICKID integers
    rootdirname : :class:`str`, optional, defaults to dr3
        Root directory for a data release...e.g. for dr3 this would be
        /global/project/projectdirs/cosmo/data/legacysurvey/dr3/

    Returns
    -------
    :class:`recarray`
         instruc with the brick information columns now populated
    """

    #ADM columns to be read in from brick file
    cols = ['BRICKNAME','BRICKID','RA','DEC','RA1','RA2','DEC1','DEC2']

    #ADM read in the brick information file
    fx = fitsio.FITS(rootdirname+'/survey-bricks.fits.gz', upper=True)
    brickdata = fx[1].read(columns=cols)
    #ADM populate the coordinate/name/ID columns
    for col in cols:
        instruc[col] = brickdata[brickids-1][col]

    #ADM read in the data-release specific
    #ADM read in the brick information file
    fx = fitsio.FITS(glob(rootdirname+'/survey-bricks-dr*.fits.gz')[0], upper=True)
    ebvdata = fx[1].read(columns=['BRICKNAME','NEXP_G','NEXP_R','NEXP_Z','EBV'])

    #ADM as the BRICKID isn't in the dr-specific file, create
    #ADM a look-up dictionary to match indices via a list comprehension
    orderedbricknames = instruc["BRICKNAME"]
    dd = defaultdict(list)
    for index, item in enumerate(ebvdata["BRICKNAME"]):
        dd[item].append(index)
    matches = [index for item in orderedbricknames for index in dd[item] if item in dd]

    #ADM populate E(B-V) and NEXP
    instruc['NEXP_G'] = ebvdata[matches]['NEXP_G']
    instruc['NEXP_R'] = ebvdata[matches]['NEXP_R']
    instruc['NEXP_Z'] = ebvdata[matches]['NEXP_Z']
    instruc['EBV'] = ebvdata[matches]['EBV']

    return instruc


def populate_depths(instruc,rootdirname='/global/project/projectdirs/cosmo/data/legacysurvey/dr3/'):
    """Add depth-related information to a numpy array

    Parameters
    ----------
    instruc : :class:`recarray`
        numpy structured array containing at least
        ['BRICKNAME','BRICKAREA','PSFDEPTH_G','PSFDEPTH_R','PSFDEPTH_Z',
        'GALDEPTH_G','GALDEPTH_R','GALDEPTH_Z','PSFDEPTH_G_PERCENTILES',
        'PSFDEPTH_R_PERCENTILES','PSFDEPTH_Z_PERCENTILES','GALDEPTH_G_PERCENTILES',
        'GALDEPTH_R_PERCENTILES','GALDEPTH_Z_PERCENTILES']
        to populate with depths and areas
    rootdirname : :class:`str`, optional, defaults to dr3
        Root directory for a data release...e.g. for dr3 this would be
        /global/project/projectdirs/cosmo/data/legacysurvey/dr3/

    Returns
    -------
    :class:`recarray`
         instruc with the per-brick depth and area columns now populated
    """
    #ADM the pixel scale area for a brick (in sq. deg.)
    pixtodeg = 0.262/3600./3600.

    #ADM read in the brick depth file
    fx = fitsio.FITS(glob(rootdirname+'*depth.fits.gz')[0], upper=True)
    depthdata = fx[1].read()

    #ADM construct the magnitude bin centers for the per-brick depth
    #ADM file, which is expressed as a histogram of 50 bins of 0.1mag
    magbins = np.arange(50)*0.1+20.05
    magbins[0] = 0

    #ADM percentiles at which to assess the depth
    percs = np.array([10,25,50,75,90])/100.

    #ADM lists to contain the brick names and for the depths, areas, percentiles
    names, areas = [], []
    depth_g, depth_r, depth_z = [], [], []
    galdepth_g, galdepth_r, galdepth_z= [], [], []
    perc_g, perc_r, perc_z = [], [], []
    galperc_g, galperc_r, galperc_z = [], [], []

    #ADM build a per-brick weighted depth. Also determine pixel-based area of brick.
    #ADM the per-brick depth file is histogram of 50 bins
    #ADM this grew organically, could make it more compact
    for i in range(0,len(depthdata),50):
        #ADM there must be measurements for all of the pixels in one band
        d = depthdata[i:i+50]
        totpix = sum(d['COUNTS_GAL_G']),sum(d['COUNTS_GAL_R']),sum(d['COUNTS_GAL_Z'])
        maxpix = max(totpix)
        #ADM percentiles in terms of pixel counts
        pixpercs = np.array(percs)*maxpix
        #ADM add pixel-weighted mean depth
        depth_g.append(np.sum(d['COUNTS_PTSRC_G']*magbins)/maxpix)
        depth_r.append(np.sum(d['COUNTS_PTSRC_R']*magbins)/maxpix)
        depth_z.append(np.sum(d['COUNTS_PTSRC_Z']*magbins)/maxpix)
        galdepth_g.append(np.sum(d['COUNTS_GAL_G']*magbins)/maxpix)
        galdepth_r.append(np.sum(d['COUNTS_GAL_R']*magbins)/maxpix)
        galdepth_z.append(np.sum(d['COUNTS_GAL_Z']*magbins)/maxpix)
        #ADM add name and pixel based area of the brick
        names.append(depthdata['BRICKNAME'][i])
        areas.append(maxpix*pixtodeg)
        #ADM add percentiles for depth...using a
        #ADM list comprehension, which is fast because the pixel numbers are ordered and
        #ADM says "give me the first magbin where we exceed a certain pixel percentile"
        if totpix[0]:
            perc_g.append([ magbins[np.where(np.cumsum(d['COUNTS_PTSRC_G']) > p )[0][0]] for p in pixpercs ])
            galperc_g.append([ magbins[np.where(np.cumsum(d['COUNTS_GAL_G']) > p )[0][0]] for p in pixpercs ])
        else:
            perc_g.append([0]*5)
            galperc_g.append([0]*5)

        if totpix[1]:
            perc_r.append([ magbins[np.where(np.cumsum(d['COUNTS_PTSRC_R']) > p )[0][0]] for p in pixpercs ])

            galperc_r.append([ magbins[np.where(np.cumsum(d['COUNTS_GAL_R']) > p )[0][0]] for p in pixpercs ])
        else:
            perc_r.append([0]*5)
            galperc_r.append([0]*5)

        if totpix[2]:
            perc_z.append([ magbins[np.where(np.cumsum(d['COUNTS_PTSRC_Z']) > p )[0][0]] for p in pixpercs ])
            galperc_z.append([ magbins[np.where(np.cumsum(d['COUNTS_GAL_Z']) > p )[0][0]] for p in pixpercs ])
        else:
            perc_z.append([0]*5)
            galperc_z.append([0]*5)

    #ADM HACK HACK HACK
    #ADM first find bricks that are not in the depth file and populate them
    #ADM with nonsense. This is a hack as I'm not sure why such bricks exist
    #ADM HACK HACK HACK
    orderedbricknames = instruc["BRICKNAME"]
    badbricks = np.ones(len(orderedbricknames))
    dd = defaultdict(list)
    for index, item in enumerate(orderedbricknames):
        dd[item].append(index)
    matches = [index for item in names for index in dd[item] if item in dd]
    badbricks[matches] = 0
    w = np.where(badbricks)
    badbricknames = orderedbricknames[w]
    for i, badbrickname in enumerate(badbricknames):
        names.append(badbrickname)
        areas.append(-99.)
        depth_g.append(-99.)
        depth_r.append(-99.)
        depth_z.append(-99.)
        galdepth_g.append(-99.)
        galdepth_r.append(-99.)
        galdepth_z.append(-99.)
        perc_g.append([-99.]*5)
        perc_r.append([-99.]*5)
        perc_z.append([-99.]*5)
        galperc_g.append([-99.]*5)
        galperc_r.append([-99.]*5)
        galperc_z.append([-99.]*5)

    #ADM, now order the brickname to match the input structure using
    #ADM a look-up dictionary to match indices via a list comprehension
    orderedbricknames = instruc["BRICKNAME"]
    dd = defaultdict(list)
    for index, item in enumerate(names):
        dd[item].append(index)
    matches = [index for item in orderedbricknames for index in dd[item] if item in dd]

    #ADM populate the depths and area
    instruc['BRICKAREA'] = np.array(areas)[matches]
    instruc['PSFDEPTH_G'] = np.array(depth_g)[matches]
    instruc['PSFDEPTH_R'] = np.array(depth_r)[matches]
    instruc['PSFDEPTH_Z'] = np.array(depth_z)[matches]
    instruc['GALDEPTH_G'] = np.array(galdepth_g)[matches]
    instruc['GALDEPTH_R'] = np.array(galdepth_r)[matches]
    instruc['GALDEPTH_Z'] = np.array(galdepth_z)[matches]
    instruc['PSFDEPTH_G_PERCENTILES'] = np.array(perc_g)[matches]
    instruc['PSFDEPTH_R_PERCENTILES'] = np.array(perc_r)[matches]
    instruc['PSFDEPTH_Z_PERCENTILES'] = np.array(perc_z)[matches]
    instruc['GALDEPTH_G_PERCENTILES'] = np.array(galperc_g)[matches]
    instruc['GALDEPTH_R_PERCENTILES'] = np.array(galperc_r)[matches]
    instruc['GALDEPTH_Z_PERCENTILES'] = np.array(galperc_z)[matches]

    return instruc


def brick_info(targetfilename,rootdirname='/global/project/projectdirs/cosmo/data/legacysurvey/dr3/',outfilename='brick-info-dr3.fits'):
    """Create a file containing brick information (depth, ebv, etc. as in construct_QA_file)

    Parameters
    ----------
    targetfilename : :class:`str`
        File name of a list of targets created by select_targets
    rootdirname : :class:`str`, optional, defaults to dr3
        Root directory for a data release...e.g. for dr3 this would be
        /global/project/projectdirs/cosmo/data/legacysurvey/dr3/
    outfilename: :class:`str`
        Output file name for the brick_info file, which will be written as FITS

    Returns
    -------
    :class:`recarray`
         numpy structured array of brick information with columns as in construct_QA_file
    """

    start = time()

    #ADM read in target file
    print('Reading in target file...t = {:.1f}s'.format(time()-start))
    fx = fitsio.FITS(targetfilename, upper=True)
    targetdata = fx[1].read(columns=['BRICKID','DESI_TARGET','BGS_TARGET','MWS_TARGET'])

    print('Determining unique bricks...t = {:.1f}s'.format(time()-start))
    #ADM determine number of unique bricks and their integer IDs
    brickids = np.array(list(set(targetdata['BRICKID'])))
    brickids.sort()

    print('Creating output brick structure...t = {:.1f}s'.format(time()-start))
    #ADM set up an output structure of size of the number of unique bricks
    nbricks = len(brickids)
    outstruc = construct_QA_file(nbricks)

    print('Adding brick information...t = {:.1f}s'.format(time()-start))
    #ADM add brick-specific information based on the brickids
    outstruc = populate_brick_info(outstruc,brickids,rootdirname)

    print('Adding depth information...t = {:.1f}s'.format(time()-start))
    #ADM add per-brick depth and area information
    outstruc = populate_depths(outstruc,rootdirname)

    print('Adding target density information...t = {:.1f}s'.format(time()-start))
    #ADM bits and names of interest for desitarget
    #ADM -1 as a bit will return all values
    bitnames = ["DENSITY_ALL","DENSITY_LRG","DENSITY_ELG",
                "DENSITY_QSO","DENSITY_BGS","DENSITY_MWS"]
    bitvals = [-1]+list(2**np.array([0,1,2,60,61]))

    #ADM loop through bits and populate target densities for each class
    for i, bitval in enumerate(bitvals):
        w = np.where(targetdata["DESI_TARGET"] & bitval)
        if len(w[0]):
            targsperbrick = np.bincount(targetdata[w]['BRICKID'])
            outstruc[bitnames[i]] = targsperbrick[outstruc['BRICKID']]/outstruc['BRICKAREA']

    print('Writing output file...t = {:.1f}s'.format(time()-start))
    #ADM everything should be populated, just write it out
    fitsio.write(outfilename, outstruc, extname='BRICKINFO', clobber=True)

    print('Done...t = {:.1f}s'.format(time()-start))
    return outstruc
